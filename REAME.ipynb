{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Finding Lane Lines on the Road** \n",
    "[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n",
    "\n",
    "<img src=\"examples/laneLines_thirdPass.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    "\n",
    "## 1. Overview\n",
    "**Finding Lane Lines on the Road**\n",
    "\n",
    "How does the algorithm recognize the lane lines?\n",
    "\n",
    "First, we must understand how do human recognize the lane lines. There are many ways we can characterize the lane lines, however for most people we intutively recognize the lane lines by features like color, position and shape, just like how we learn to recognize any other objects or person.\n",
    "\n",
    "To recognize the lane lines, we can apply the same techniques to teach the computer to do the same. Naturally we can treat the things we see with our eyes as a video stream or sets of moving images. And for computers to perceive the moving images like us, we need to undertand the information stored in the image and how can we extract useful features from it such as edge detection using Canny Edge detection algorithm, as well as finding lines from the detected edges using Hough transform algorithm.\n",
    "\n",
    "## 2. Goal\n",
    "The goal of this lane finding project is to track the lane lines on the road and draw the lines over the lane lines on the road. To do that, a series of computer vision pipelines (using openCV) is built to analyze the video stream.\n",
    "\n",
    "\n",
    "## 3. Test Video Samples and Pipeline Output\n",
    "Three test video samples are used to verify the lane detection pipeline.\n",
    "\n",
    "1. [Basic]        : **Road with white lines only** \n",
    "The first case is the video with the white lines only, it is considered as base case because to the white line features are easily identifiable on a dark color road pavement\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Input:  Road with White Lines </td>\n",
    "    <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Output: Road with White Lines </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> <video width=\"100%\" controls src=\"test_videos/solidWhiteRight.mp4\"/> </td>\n",
    "    <td> <video width=\"100%\" controls src=\"test_videos_output/solidWhiteRight.mp4\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "2. [Intermediate] : **Road with white and yellow lines** \n",
    "The second case with both white and yellow lines are slightly more difficult for the pipeline to extract the lane line features (on a smooth dark road pavement)\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Input:  Road with White & Yellow Lines </td>\n",
    "    <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Output: Road with White & Yellow Lines </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> <video width=\"100%\" controls src=\"test_videos/solidYellowLeft.mp4\"/> </td>\n",
    "    <td> <video width=\"100%\" controls src=\"test_videos_output/solidYellowLeft.mp4\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "3. [Advanced]     : **Light color pavement and mixed shades on the road** \n",
    "Lastly, the most challenging case is the extras of the mixed of the shares on the road, the light color road pavement (that makes tracks yellow line filtering difficult), curve lines and shades on the road.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Input:  Light Color Pavement and Shades </td>\n",
    "    <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Output: Light Color Pavement and Shades </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> <video width=\"100%\" controls src=\"test_videos/challenge.mp4\"/> </td>\n",
    "    <td> <video width=\"100%\" controls src=\"test_videos_output/challenge.mp4\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "## 4. Image and Color Space\n",
    "\n",
    "\n",
    "### Image, Color Space (RGB, HSV, HLS) and Color Masks \n",
    "\n",
    "- Image data is made up number of pixels of different colors in 3 RGB color layers (Red,Green,Blue)\n",
    "- Image file stores the color-pixel in stacks of arrays, each for different color channel.\n",
    "\n",
    "### How do color space and color masks relate to finding lane lines and why they are important? \n",
    "\n",
    "Since images are stored in different color channels, it makes it easy to find the white and yellow lines or lanes if we can consider only the white and yellow pixels in the image. Here is a good example on using [OpenCV and Python for Color Detection](https://www.pyimagesearch.com/2014/08/04/opencv-python-color-detection/)\n",
    "\n",
    "There are many different methods to filter/extract the colors from the image, images can be represented in a number of different color space such as RGB, HSV and HLS. \n",
    "\n",
    "More info can be found on [Color Space on Wiki](https://en.wikipedia.org/wiki/Color_space).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 5. Lane Detection Pipeline\n",
    "\n",
    "\n",
    "1. **Color Filtering** - `select_color_mask()` Select white and yellow color by using color masks (RGB/HSV/HLS). * **HLS** * color mask yields the most consistent color filtering in this project\n",
    "\n",
    "\n",
    "2. **Grayscale** - `grayscale()`  Convert the image color to grayscale for Canny edge detection \n",
    "\n",
    "\n",
    "3. **Blurring** - `gaussian_blur()`  Blur the image using Gaussian blur transformation for smoother edge detection \n",
    "\n",
    "\n",
    "4. **Edge detection**  - `canny()`  Detect edges using Canny edge detection algorithm \n",
    "\n",
    "\n",
    "5. **Region of Interest**\n",
    "    1. `select_polygon_vertices()`  Select the region of interest for the detected edges\n",
    "    2. `select_masked_edges()`  Mask the detected edges with region of interest\n",
    " \n",
    "6. **Hough Lines** `select_hough_lines()`  Apply Hough Lines transform to find lines using masked edges\n",
    "\n",
    "7. **Paritioning the lines & Averaging and Extrapolating Lines**\n",
    "    1. **Paritioning the lines** `partition_lines()`  Paritioning the lines to find left and right lanes\n",
    "    2. **Averaging and Extrapolating lines** `avg_extrapolated_lines()`  Apply average and extra-polate algorithm (linear regression / polyfit) to find a straight line to track the lane line\n",
    "\n",
    "8. **Process images in series** `process_video()` Apply all of the above image processing steps `process_image()` with a series of images using `moviepy.editor`\n",
    "\n",
    "During the development of the pipeline, it is tested on a number of test images for verifications of the lane tracking performance under various conditions and parameters tuning. Once the output of the test images are satisified, the pipeline is followed by testing on a number of test videos to verify the lane line tracking in real-time.\n",
    "\n",
    "Initially when the `draw_lines()` function was used, the lane line finding pipeline was only able to draw the line segments that were identified by Hough Transform (from the output of the Canny edges).\n",
    "\n",
    "In order to improve the robustness of the lane line finding and able to mark the lanes clearly with a solid line, two additional stages are added to enhance the lane finding pipeline:\n",
    "\n",
    "- Partitioning the left / right lane lines based on the line slope\n",
    "- Approximation of a straight line that fits to each of the left / right lane lines\n",
    "\n",
    "#### Choosing the Color Masks\n",
    "<center>RGB vs HLS Mask Comparison #1: Yellow Lines video (t = 11.6s)</center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"report_images/rgb_vs_hls_mask/solidYellowLeft_rgb_time_11.6s.jpg\" alt=\"Drawing\"/> </td>\n",
    "        <td> <img src=\"report_images/rgb_vs_hls_mask/solidYellowLeft_hls_time_11.6s.jpg\" alt=\"Drawing\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> RGB color mask is having difficulties retaining the yellow pixel </td>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> HLS color mask robustly filter the yellow pixel </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**<center> <font color = 'green'> HSL color mask wins for consistent filtering yellow pixels </font> </center>**\n",
    "\n",
    "In the video with yellow line, the HSL color mask performs better than the RGB because it is able to consistently filters smooth yellow pixels than RGB color mask, despite it sometimes picks up  unwanted noises such as yellow road sign and the lawn on the right. However, these minor imperfections of HLS mask should not be a problem, as later we can apply ```select_region_of_interest()``` to specifically crop the non-road sections.\n",
    "\n",
    "\n",
    "**<center> RGB vs HLS Mask Comparison #2: Challenge video (t = 4.7s)</center>**\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"report_images/rgb_vs_hls_mask/challenge_time_4.7s_rgb_no_lines.jpg\" alt=\"Drawing\"/> </td>\n",
    "        <td> <img src=\"report_images/rgb_vs_hls_mask/challenge_time_4.7s_hls_has_lines.jpg\" alt=\"Drawing\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> RGB color mask could not show lines </td>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> HLS color mask shows lines </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**<center><font color = 'green'> HSL color mask wins because it robustly filters for the yellow line even under tree shades and light color pavement </font></center>**\n",
    "\n",
    "In the challenge video with light color road pavement, the HSL color mask performs much better than the RGB because it is able to consistently filter smooth yellow pixels than RGB color mask even under a very low contrast with some part of the road with very\n",
    "light color pavement. As mentioned earlier, th minor imperfections of HLS mask should not be a problem, as later we can apply ```select_region_of_interest()``` to specifically crop the non-road sections.\n",
    "\n",
    "Bear in mind that color filtering is the important first step for this pipeline to work. Without robust approriate color filtering, the yellow lines will not be available as an input for the Canny edege detection and causes the lane detection pipeline to fail.\n",
    "\n",
    "#### Partitioning the Lines that are not Horizontal\n",
    "\n",
    "While partitioning the lane lines to left and right lanes by their graidents is useful to feed the line data to the straight line appoximation. It is a great opportunity to remove any unwanted lines which are not lane lines, for example *** rejecting *** any horizontal lines which have less than 15 degrees since the lane lines can never be horizontal lines (i.e. line gradient close to zero).\n",
    "\n",
    "Below are the comparison of implementing the horizontal lines rejection by specifying 15 degrees angles to ignore the lines.\n",
    "\n",
    "\n",
    "**<center> Horizontal Lines Rejection #1: Yellow Line Video (t = 11.52s)</center>**\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"test_videos_output/horizontal_lines/left_white_horizontal_line_problem/solidYellowLeft_time_11.52s_start_NOK.jpg\" alt=\"Drawing\"/> </td>\n",
    "        <td> <img src=\"test_videos_output/horizontal_lines/left_white_horizontal_line_OK/solidYellowLeft_time_11.52.jpg\" alt=\"Drawing\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Without rejecting the horizontal lines </td>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> <bold>With rejecting</bold> the horizontal lines </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**<center> Horizontal Lines Rejection #2: Yellow Line Video (t = 14.76s)</center>**\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"test_videos_output/horizontal_lines/right_white_horizontal_line_problem/solidYellowLeft_time_14.76s_start_NOK.jpg\" alt=\"Drawing\"/> </td>\n",
    "        <td> <img src=\"test_videos_output/horizontal_lines/right_white_horizontal_line_OK/solidYellowLeft_time_14.76.jpg\" alt=\"Drawing\"/> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> Without rejecting the horizontal lines </td>\n",
    "        <td style=\"text-align: center; vertical-align: middle;\" bgcolor=\"white\"> <bold> With rejecting</bold> the horizontal lines </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Averaging and Extrapolating the lines\n",
    "\n",
    "After the lines have been partitioned by `partition_lines()`, we can create the best line of fit model to help us to map out the full extent of the lane lines by predicting the x coordinate of the lane line for a given y coordinate.\n",
    "\n",
    "There are more than one way to implement the best line of fit for a straight line prediction. In this project, it is chosen to use two popular straight line model library:\n",
    "\n",
    "- Scikit-learn `LinearRegression()` as linear regression:\n",
    "    - `avg_extrapolated_lines_linear_reg()`\n",
    "\n",
    "\n",
    "- Numpy's polyfit `np.polyfit()` as 1st order polynomial:\n",
    "    - `avg_extrapolated_lines_polyfit()`\n",
    "\n",
    "Both averaging and extrapolation methods were tested and work equally well. Scikit-learn `LinearRegresion()` is chosen as the preferred method to predict the line for the lane lines.\n",
    "\n",
    "The lines are drawn from the bottom of the image and extend out to the top of the region of interest as the lane lines eventually emerge towards a single point.\n",
    "\n",
    "#### Design of Region of Interest\n",
    "\n",
    "Implementing the region of interest in a computer vision pipeline has two benefits:\n",
    "\n",
    "- Improved accuracy by shortlisting important features for processing\n",
    "- Improved pipeline efficiency by eliminating unwanted data to the next stage in the pipeline\n",
    "\n",
    "Region of interest can be shape of any kind. In this project, a 4 sides polygon is chosen to encapsulate the left and right lane lines. The vertices of the polygon are chosen to shape like a trapezoid and the bottom base will be parallel to the bottom of the image like:\n",
    "\n",
    "    #------------------------------------------------#\n",
    "    #        Define vertices of the polygon mask     #\n",
    "    #        to draw trapezoid shape                 #\n",
    "    #                      -------                   #\n",
    "    #                     /       \\                  #\n",
    "    #                    /         \\                 #\n",
    "    #                   /-----------\\                #\n",
    "    #------------------------------------------------#\n",
    "\n",
    "The vertices coordinates of the polygon is calculated using `select_polygon_vertices()` with parameters to adjust `img_border_pct` - the image border offset of the ploygon as a percentage, and `height_pct` - the vertical offset from the top of the image as a percentage. \n",
    "\n",
    "These parameters are calibrated to work for all the test images and the test_videos (including the optional challenge).\n",
    "\n",
    "### 6. Potential Shortcomings with the Pipeline\n",
    "\n",
    "#### Pipeline Design Consideration\n",
    "\n",
    "As with any programs or pipelines, there are often cases where exists potential shortcomings on the accuracy and performance of the functionalities. A well design pipeline should be **generic enough** to deal with new conditions ***but*** **specific enough** to target on intended operating conditions.\n",
    "\n",
    "#### Robustness vs Stabilisation Tradeoff - Image Pre-processing Parameters Tuning\n",
    "\n",
    "The performance and robustness of the lane lines finding pipeline to find the lines using Hough transform `cv2.HoughLinesP()` which are largely dependent on its input: the quality of the pre-processing image, this is based on selection of color filtering, blurring, edge detection, region of interest.\n",
    "\n",
    "One potential shortcoming is the tradeoff of the level of preprocessing on the tracking performance to robustness of the overall pipeline. In order to ensure robustness of  finding lane lines works for every video, it is cruical to have consistent edge detection at every video, else there will be no edges to find lines and the pipeline will fail.\n",
    "\n",
    "For example, aggressively pre-processing the image tend to filter out more unwanted noises on the image which makes the lane line tracking less jittery but at the same time also lessen the sensitivity of detecting edges. When an aggresive pre-processing setting is chosen, the chances of no edges detection increases and imposes risk of the pipeline failing.\n",
    "\n",
    "Therefore, careful selection of parameters for color filtering, blurring, edge detection, region of interest are chosen on conservative values while maintaining reasonable lane finding stabilisation.\n",
    "\n",
    "#### Lighting conditions\n",
    "\n",
    "Another shortcoming is the light contrast or shade variations on the road (as seen in the `test_videos/Challenge.mp4` video), which makes the lane finding algorithm difficult to filter/distinguish the lane lines from the road.\n",
    "\n",
    "The pipeline works with limited lighting conditions (i.e. day time) with high contrast, so that the white and yellow lines can filtered from the image. The pipeline is likely to struggle to work on low light conditions (i.e. at dusk or at night).\n",
    "\n",
    "#### Camera / Lens Specific\n",
    "\n",
    "The pipeline will work well under the assumption of running it with the same dash camera that was used to calibrate/tune used parameters in `process_image()` because **camera resolution and camera colors** varies amongst camera. Color filtering parameters may need to adjust according to adapt to specific camera application.\n",
    "\n",
    "#### Camera Position and Orientation\n",
    "\n",
    "**Camera orientation and the mounting location** also play an important factor in pipeline because it affects the selection of the region of interest (ROI), which is a great tool used to crop out the non-road sections from the image before the pipeline identifies the lines from the (ROI) masked detected edges. \n",
    "\n",
    "All the test images and videos are captured in landscape, so if the camera would have mounted in portrait, then the ROI mask needs to be rotated accordingly to compensate for the **Hough transform** and the **lines partitioning** to work\n",
    "\n",
    "The camera mounting location also greatly affects the robustness of the lane finding pipeline because different locations may include unwanted objects such as the engine cover at the bottom of the video, as seen in the `Challenge.mp4` video. Therefore tuning of vertical offset for the ROI mask is required, unfortunately, there is no one fit for all solution for optimum vertical offset and need to calibrated manually.\n",
    "\n",
    "\n",
    "#### Large Curvature Lanes\n",
    "\n",
    "Currently the lane line finding pipeline assumes the lane lines in the video are straight line and will work well also for near-straight curve lines. However, it is anticpated that it will not working well if the curve is too large then the linear model will no longer work well, which means the linear regression mode will probably needs to be replaced with a 2nd or 3rd order polynominl fitting model to predict the lane lines on the curve.\n",
    "    \n",
    "#### Lane tracking Stability for Road bumps\n",
    "\n",
    "The road bumps on the road may distrub the lane tracking temporarily, more algorithms need to add to the pipeline to filter the vertical/horizotal image drift or offset.\n",
    "\n",
    "### 7. Parameter Values and Effects\n",
    "\n",
    "In this pipeline, many parameters have been used to extract useful features during the pre-processing, edge detection and line detection. Some parameters are more sensitive than others for the edge/line feature extractions. Loosely speaking the parameters used in the earlier part of the pipeline tends to have more influence to the lane line tracking output.\n",
    "\n",
    "Through experimentation of trying different parameters, it was found that these group of parameters have a primary impact in the lane line tracking performance:\n",
    "\n",
    "1. **Color Mask Thresholds** : \n",
    "The lower and upper threshold affects on the sensivity in selecting the white and yellow colors. If the range between these two thresholds is too small, then this may require the Canny edge detection thresholds to be more sensitive, vice versa.\n",
    "\n",
    "\n",
    "2. **Gaussian Kernel Size**: \n",
    "The kernal size is a number dicate the size of the Gassuian kernal array and this number must be an odd number. Kernel size of 3 and 5 are are commonly used. For this project, a relatively large kernel size of 15 is chosen to blur the image more to result in a fewer edges detection for background noise rejection reason.\n",
    "\n",
    "\n",
    "3. **Canny Edge Detection Threshold** : \n",
    "John F. Canny, the inventor of the edge detection recommends the optimal ratio of the lower and upper thresholds ideally should be set around 1:2 or 1:3 for the edge detection to to work. In general, smaller values in the lower and upper thresholds tend to be less sensitive in recognizing the edges.\n",
    "\n",
    "After setting the paramters in the HLS color mask thresholds and Gaussian kernel, Canny edge threshold ratio of 1:3 is chosen and then an optimal threshold values are identified after some trial and errors.\n",
    "\n",
    "4. **Hough Lines Threshold** \n",
    "\n",
    "Apart from getting the edge detection right, the Hough line parameters also play an important role to convert the edges to line segments using the [`cv2.HoughLinesP()`](https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=houghlinesp#houghlinesp)\n",
    "\n",
    "    rho          : Distance resolution in pixels of the Hough grid\n",
    "    theta        : Angular resolution in radians of the Hough grid\n",
    "    threshold    : Minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_len : Minimum number of pixels making up a line\n",
    "    max_line_gap : Maximum allowed gap in pixels between connectable line segments\n",
    "\n",
    "\n",
    "Smaller `rho`, `theta` and `threshold` values make more line segments to be recognized. These parameters were tuned to being able to just registering the lane line segments. To make tuning of these paramters easier, `rho` and `thea` are set to 1 and `threshold` were set from low values and gradually to increase until the line segments for the lane lines cannot be recognized.\n",
    "\n",
    "The `min_line_len` parameter specifies the minimum length of the line to the output, essentially it rejects any line segments that are shorter than this minimum line length. For this project, it was calibrated to reject the random lines in the image, for example the thin horizontal lines appeared in the test video.\n",
    "\n",
    "The `max_line_gap` parameter specifies the sensitivity of connecting the line segments together, higher value makes the line segments to easier to connect and form a single line. A large value is chosen in this project to ensure the line segments are connected and results fewer lines output, this helps to reduce the variations of the lines and jittering while finding the lanelines.\n",
    "\n",
    "[Effect of the Canny edge detection and Houghline Transform parameters also discussed here](https://dsp.stackexchange.com/questions/10467/influence-of-image-size-to-edge-detection-in-opencv)\n",
    "\n",
    "5. **Vertices of the polygon mask: border and height of the mask**\n",
    "\n",
    "The vertices of the polygon mask is calculated based on the user specified parameters of the inner border of the image and the ratio of the height of the mask to the image height. The selection of these parameters help to filter out unwanted edges to be included for the linear lane line model prediction.\n",
    "\n",
    "6. **Line angles to ignore **\n",
    "\n",
    "The `ignore_angle_degrees` angles to ignore parameter is served as rejecting non lane lines recognized by the Canny edge detection and the Hough line transform. As seen in the `solidYellowLeft.mp4` video, at times there is a small horizontal white line marking showing in the test video. Without the implementation of ignoring the line angles greater than non-zero angle, then this horizontal line is unexpectedly added to the pool of data to estimate the line of fit. In turn causes the lane line marking momentarily predict the incorrect the lane line.\n",
    "\n",
    "\n",
    "### 8.  Conclusions\n",
    "\n",
    "The lane finding pipeline was able to successfully to track the white and yellow lane lines well with little to no jittering, as seen in all the videos in the `test_videos_output` folder.\n",
    "\n",
    "The pipeline was primarily constructed based on a series of computer vision libraries, followed by line fitting model and video editing libraries.\n",
    "\n",
    "Approriate selection of the color masks, canny edge detection and hough line transform parameters are important for filtering the image background and extracting the line segments correctly. Based on the the line segments, the line fitting model was used to find the best line of fit to find the lane lines.\n",
    "\n",
    "\n",
    "### 9.  Future Work\n",
    "\n",
    "The above pipeline demonstrated the concept of finding lanes using simple computer vision techniques operate in certain conditions. There are still areas in the pipeline that can be improved such as better lane tracking for curves, able to work in low/no light condition, lane line tracking stability and robustness in the field.\n",
    "\n",
    "#### Polynomial Fit to the Curve\n",
    "\n",
    "Instead of using the linear regression model to predict the lane lines, a possible improvement could be using a 2nd or 3rd polynomial fit for the Hough lines.\n",
    "\n",
    "#### Adaptable Parameters for Different Light Conditions\n",
    "\n",
    "Currently the pipeline works well with good lighting conditions, a possible improvment to make the pipeline more robustness to a more diverse operating condition could be adding to a stage to the pipeline to replace some fixed parameters used the color filtering parameters, the edge detection and houghline transform with variables.\n",
    "\n",
    "#### Adjust the Region of Interests based on the Landscape or Portrait Mode\n",
    "\n",
    "Currently the pipeline only works when the video is in landscape, a posssible improvement is to create a function to identify if the input is landscape or portrait by the height and width dimensions and adjust the region of interest mask vertices accordingly.\n",
    "\n",
    "#### Deep Learning Approach to Optimize Parameters\n",
    "\n",
    "Rather than manually choosing many parameters such as thresholds used in the color mask, canny edge and hough transform for limited set of test images and video samples, it will be more effective to train a deep neural networks to find these optimum parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
